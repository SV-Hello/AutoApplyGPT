======================================================================
AutoApplyGPT Training with LoRA
======================================================================

ðŸ“‹ Loading configurations...
ðŸ“¦ Loading model: /scratch/user/u.sv309862/689csce/model
âœ… LoRA applied:
  Trainable params: 0.00M (0.00%)
  All params: 2.01B

ðŸ”§ Setting up LoRA...
âœ… LoRA applied:
  Trainable params: 3.15M (0.16%)
  All params: 2.01B
ðŸ“‚ Loading dataset from ../../data/autoapply_training/hf_dataset
Dataset sizes:
  train: 2335 examples
  validation: 291 examples
  test: 293 examples
ðŸ”„ Tokenizing dataset...
âœ… Dataset preprocessed

ðŸš€ Creating trainer...

======================================================================
Starting training...
======================================================================

{'loss': 2.3825, 'learning_rate': 0.00019999383841535888, 'epoch': 0.07}
{'loss': 2.3631, 'learning_rate': 0.00019925536657300734, 'epoch': 0.14}
{'loss': 2.2742, 'learning_rate': 0.00019729499707593284, 'epoch': 0.21}
{'loss': 2.2054, 'learning_rate': 0.00019413686333851464, 'epoch': 0.27}
{'loss': 2.1448, 'learning_rate': 0.0001898198440263633, 'epoch': 0.34}
{'loss': 2.1581, 'learning_rate': 0.0001843970844348421, 'epoch': 0.41}
{'loss': 2.1118, 'learning_rate': 0.00017793534223616353, 'epoch': 0.48}
{'loss': 2.0455, 'learning_rate': 0.00017051416564933675, 'epoch': 0.55}
{'loss': 1.9868, 'learning_rate': 0.00016222491415022507, 'epoch': 0.62}
{'loss': 1.968, 'learning_rate': 0.00015316963377740437, 'epoch': 0.68}
{'eval_loss': 1.957863688468933, 'eval_runtime': 52.6618, 'eval_samples_per_second': 5.526, 'eval_steps_per_second': 1.386, 'epoch': 0.68}
{'loss': 1.9304, 'learning_rate': 0.00014345980087953405, 'epoch': 0.75}
{'loss': 1.9214, 'learning_rate': 0.00013321494976952132, 'epoch': 0.82}
{'loss': 1.8203, 'learning_rate': 0.00012256120117994245, 'epoch': 0.89}
{'loss': 1.8101, 'learning_rate': 0.00011162970963538462, 'epoch': 0.96}
{'loss': 1.759, 'learning_rate': 0.00010055504885555797, 'epoch': 1.03}
{'loss': 1.7009, 'learning_rate': 8.947355506590749e-05, 'epoch': 1.1}
{'loss': 1.7223, 'learning_rate': 7.852164861064133e-05, 'epoch': 1.16}
{'loss': 1.665, 'learning_rate': 6.783415453020301e-05, 'epoch': 1.23}
{'loss': 1.6616, 'learning_rate': 5.7542642777962664e-05, 'epoch': 1.3}
{'loss': 1.6121, 'learning_rate': 4.777380850913258e-05, 'epoch': 1.37}
{'eval_loss': 1.6334255933761597, 'eval_runtime': 52.6208, 'eval_samples_per_second': 5.53, 'eval_steps_per_second': 1.387, 'epoch': 1.37}
{'loss': 1.5882, 'learning_rate': 3.8647912381595416e-05, 'epoch': 1.44}
{'loss': 1.5816, 'learning_rate': 3.0277300069550006e-05, 'epoch': 1.51}
{'loss': 1.547, 'learning_rate': 2.2765019215716464e-05, 'epoch': 1.58}
{'loss': 1.5735, 'learning_rate': 1.6203550848313775e-05, 'epoch': 1.64}
{'loss': 1.5374, 'learning_rate': 1.06736708798844e-05, 'epoch': 1.71}
{'loss': 1.5211, 'learning_rate': 6.243455703651369e-06, 'epoch': 1.78}
{'loss': 1.5636, 'learning_rate': 2.9674441291587806e-06, 'epoch': 1.85}
{'loss': 1.5416, 'learning_rate': 8.85965974309011e-07, 'epoch': 1.92}
{'loss': 1.5288, 'learning_rate': 2.4645579261994844e-08, 'epoch': 1.99}
{'train_runtime': 2877.8138, 'train_samples_per_second': 1.623, 'train_steps_per_second': 0.101, 'train_loss': 1.832622227603442, 'epoch': 2.0}

ðŸ’¾ Saving final model...
âœ… Model saved to ../../data/autoapply_training/outputs/final_model

ðŸ“Š Evaluating on test set...

======================================================================
Test Results:
  eval_loss: 1.6233
  eval_runtime: 53.1092
  eval_samples_per_second: 5.5170
  eval_steps_per_second: 1.3930
  epoch: 2.0000
======================================================================

âœ… Training complete!

ðŸŽ‰ All done!
