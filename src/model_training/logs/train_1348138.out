======================================================================
AutoApplyGPT Training with LoRA
======================================================================

ðŸ“‹ Loading configurations...
ðŸ“¦ Loading model: /scratch/user/u.sv309862/689csce/model
âœ… LoRA applied:
  Trainable params: 0.00M (0.00%)
  All params: 2.01B

ðŸ”§ Setting up LoRA...
âœ… LoRA applied:
  Trainable params: 3.15M (0.16%)
  All params: 2.01B
ðŸ“‚ Loading dataset from ../../data/autoapply_training/hf_dataset
Dataset sizes:
  train: 2335 examples
  validation: 291 examples
  test: 293 examples
ðŸ”„ Tokenizing dataset...
âœ… Dataset preprocessed

ðŸš€ Creating trainer...

======================================================================
Starting training...
======================================================================

{'loss': 2.3826, 'learning_rate': 0.00019999383841535888, 'epoch': 0.07}
{'loss': 2.3638, 'learning_rate': 0.00019925536657300734, 'epoch': 0.14}
{'loss': 2.2762, 'learning_rate': 0.00019729499707593284, 'epoch': 0.21}
{'loss': 2.2067, 'learning_rate': 0.00019413686333851464, 'epoch': 0.27}
{'loss': 2.1447, 'learning_rate': 0.0001898198440263633, 'epoch': 0.34}
{'loss': 2.1582, 'learning_rate': 0.0001843970844348421, 'epoch': 0.41}
{'loss': 2.1116, 'learning_rate': 0.00017793534223616353, 'epoch': 0.48}
{'loss': 2.0449, 'learning_rate': 0.00017051416564933675, 'epoch': 0.55}
{'loss': 1.9875, 'learning_rate': 0.00016222491415022507, 'epoch': 0.62}
{'loss': 1.9688, 'learning_rate': 0.00015316963377740437, 'epoch': 0.68}
{'eval_loss': 1.9581806659698486, 'eval_runtime': 55.0142, 'eval_samples_per_second': 5.29, 'eval_steps_per_second': 1.327, 'epoch': 0.68}
{'loss': 1.9312, 'learning_rate': 0.00014345980087953405, 'epoch': 0.75}
{'loss': 1.9198, 'learning_rate': 0.00013321494976952132, 'epoch': 0.82}
{'loss': 1.8217, 'learning_rate': 0.00012256120117994245, 'epoch': 0.89}
{'loss': 1.8117, 'learning_rate': 0.00011162970963538462, 'epoch': 0.96}
{'loss': 1.7641, 'learning_rate': 0.00010055504885555797, 'epoch': 1.03}
{'loss': 1.707, 'learning_rate': 8.947355506590749e-05, 'epoch': 1.1}
{'loss': 1.7297, 'learning_rate': 7.852164861064133e-05, 'epoch': 1.16}
{'loss': 1.67, 'learning_rate': 6.783415453020301e-05, 'epoch': 1.23}
{'loss': 1.6624, 'learning_rate': 5.7542642777962664e-05, 'epoch': 1.3}
{'loss': 1.616, 'learning_rate': 4.777380850913258e-05, 'epoch': 1.37}
{'eval_loss': 1.6421741247177124, 'eval_runtime': 53.4629, 'eval_samples_per_second': 5.443, 'eval_steps_per_second': 1.365, 'epoch': 1.37}
{'loss': 1.5958, 'learning_rate': 3.8647912381595416e-05, 'epoch': 1.44}
{'loss': 1.587, 'learning_rate': 3.0277300069550006e-05, 'epoch': 1.51}
{'loss': 1.5599, 'learning_rate': 2.2765019215716464e-05, 'epoch': 1.58}
{'loss': 1.5776, 'learning_rate': 1.6203550848313775e-05, 'epoch': 1.64}
{'loss': 1.5444, 'learning_rate': 1.06736708798844e-05, 'epoch': 1.71}
{'loss': 1.532, 'learning_rate': 6.243455703651369e-06, 'epoch': 1.78}
{'loss': 1.5684, 'learning_rate': 2.9674441291587806e-06, 'epoch': 1.85}
{'loss': 1.5531, 'learning_rate': 8.85965974309011e-07, 'epoch': 1.92}
{'loss': 1.5416, 'learning_rate': 2.4645579261994844e-08, 'epoch': 1.99}
{'train_runtime': 2885.5882, 'train_samples_per_second': 1.618, 'train_steps_per_second': 0.101, 'train_loss': 1.8365779527246135, 'epoch': 2.0}

ðŸ’¾ Saving final model...
âœ… Model saved to ../../data/autoapply_training/outputs/final_model

ðŸ“Š Evaluating on test set...

======================================================================
Test Results:
  eval_loss: 1.6233
  eval_runtime: 55.3875
  eval_samples_per_second: 5.2900
  eval_steps_per_second: 1.3360
  epoch: 2.0000
======================================================================

âœ… Training complete!

ðŸŽ‰ All done!
